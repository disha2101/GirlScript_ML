{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GirlScriptML.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM6/beivrSTb1kdPfLDNhVZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/disha2101/GirlScript_ML/blob/main/GirlScriptML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O99FCOa9bg5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c2970ed-17b4-4caa-c1ec-25da7c68fdda"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3N4Y_ZYcmox"
      },
      "source": [
        ""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKhwSvZHcnl-"
      },
      "source": [
        "Author  :[Disha bahal](https://github.com/disha2101)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICTM3oXDc4H4"
      },
      "source": [
        "Train a model to learn the table of 10\n",
        " \n",
        "Equation : `y = 10*x`\n",
        "\n",
        " This is a multiline code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFuSrhRndhKL"
      },
      "source": [
        "#Data Creation\n",
        "`y=10*x`\n",
        "\n",
        "Target - > \n",
        "```\n",
        "xTrain = [0,1,2,3,4,5,...]\n",
        "\n",
        "yTrain = [0,1,0,20,30,40,50,...]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Js90rXC_d8dm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9c1291a-019f-45c6-97a0-18569c57f8e3"
      },
      "source": [
        "x = [i for i in range(21)]\n",
        "print(x)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGxWqI8xkxjr",
        "outputId": "d1afb7b4-9da2-4371-9236-865040b3e432",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y = [i for i in range(10*20+1) if i%10==0]\n",
        "print(y)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXJOQS_7kKVx",
        "outputId": "9ed91555-75c7-44f4-e233-968ea0e2da65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "xTest = x[:-5]\n",
        "yTest = y[:-5]\n",
        "print(f'''Training Data:\n",
        "\n",
        "xTrain:{xTrain}\n",
        "yTrain:{yTrain}\n",
        "\n",
        "Testing Data:\n",
        "\n",
        "xTest:{xTest}\n",
        "yTest:{yTest}\n",
        "''')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Data:\n",
            "\n",
            "xTrain:[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
            "yTrain:[0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200]\n",
            "\n",
            "Testing Data:\n",
            "\n",
            "xTest:[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
            "yTest:[0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXafANJfemd6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f432ade6-30fb-45b7-8186-9fdf334c4e26"
      },
      "source": [
        "xTrain = [i for i in range(21)] #list comprehension\n",
        "print(xTrain)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yKspK6EevdG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bca500bc-b380-4d43-d2bb-b12213f00822"
      },
      "source": [
        "for i in range(11*20):\n",
        "  print(i)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtQycNVfe-Oj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "321fddcb-fa54-4564-a11c-e6f63ab2c28b"
      },
      "source": [
        "for i in range(10*20+1):\n",
        "  print(i%10 == 0)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "True\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "True\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "True\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "True\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "True\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "True\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "True\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "True\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "True\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "True\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "True\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "True\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "True\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "True\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "True\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "True\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "True\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "True\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "True\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCO7wGergaHj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fc42ff5-f1ff-4d08-be01-a99bb375125c"
      },
      "source": [
        "yTrain = [i for i in range(10*20+1) if i%10 == 0]\n",
        "print(yTrain)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBWuXzQ5g7a7"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmwwKypEgqa-",
        "outputId": "9ac45314-f058-4ac1-c977-22175a86412d"
      },
      "source": [
        "def tempFunc(x):\n",
        "  y=10*x\n",
        "  return y\n",
        "for value in xTrain:\n",
        "  print(tempFunc(value))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "10\n",
            "20\n",
            "30\n",
            "40\n",
            "50\n",
            "60\n",
            "70\n",
            "80\n",
            "90\n",
            "100\n",
            "110\n",
            "120\n",
            "130\n",
            "140\n",
            "150\n",
            "160\n",
            "170\n",
            "180\n",
            "190\n",
            "200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbnFcCeZhDnp",
        "outputId": "866af403-1013-4f9a-afe7-f6aa761b2d74"
      },
      "source": [
        "xTrain = [i for i in range(21)] #list comprehension\n",
        "print(xTrain)\n",
        "for value in xTrain:\n",
        "  print(value)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHp85jsakgYe"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeT9sZIdkpoc"
      },
      "source": [
        "#perceptron model\n",
        "model = tf.keras.Sequential([\n",
        "                             tf.keras.layers.Dense(units=1,input_shape=[1])\n",
        "]) #unit is the number of neurons and input_shape is "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKjvTtg7msS6"
      },
      "source": [
        "model.compile(optimizer='adam',loss='mae') #sgd,rmsprop,adamax,adagrad"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIvrRfHEnG5Y",
        "outputId": "eca7fc9e-b38a-40fc-e47f-05c28e4f0e95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.fit(x=xTrain,y=yTrain,validation_data=(xTest,yTest))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 625ms/step - loss: 86.3387 - val_loss: 64.7457\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f29654f3410>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6v0yRogi0tI",
        "outputId": "348943d0-3eef-4250-cd51-d28d50975bdc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "numEpoch = 500 #training it for 5000 times\n",
        "model.fit(x=xTrain,y=yTrain,validation_data=(xTest,yTest),epochs=numEpoch)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 86.3278 - val_loss: 64.7373\n",
            "Epoch 2/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 86.3169 - val_loss: 64.7289\n",
            "Epoch 3/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 86.3060 - val_loss: 64.7206\n",
            "Epoch 4/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 86.2951 - val_loss: 64.7122\n",
            "Epoch 5/500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 86.2842 - val_loss: 64.7038\n",
            "Epoch 6/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 86.2733 - val_loss: 64.6954\n",
            "Epoch 7/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 86.2624 - val_loss: 64.6871\n",
            "Epoch 8/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 86.2515 - val_loss: 64.6787\n",
            "Epoch 9/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 86.2406 - val_loss: 64.6703\n",
            "Epoch 10/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 86.2297 - val_loss: 64.6619\n",
            "Epoch 11/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 86.2188 - val_loss: 64.6536\n",
            "Epoch 12/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 86.2079 - val_loss: 64.6452\n",
            "Epoch 13/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 86.1970 - val_loss: 64.6368\n",
            "Epoch 14/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 86.1861 - val_loss: 64.6284\n",
            "Epoch 15/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 86.1752 - val_loss: 64.6201\n",
            "Epoch 16/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 86.1643 - val_loss: 64.6117\n",
            "Epoch 17/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 86.1534 - val_loss: 64.6033\n",
            "Epoch 18/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 86.1425 - val_loss: 64.5950\n",
            "Epoch 19/500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 86.1316 - val_loss: 64.5866\n",
            "Epoch 20/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 86.1207 - val_loss: 64.5782\n",
            "Epoch 21/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 86.1098 - val_loss: 64.5698\n",
            "Epoch 22/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 86.0989 - val_loss: 64.5615\n",
            "Epoch 23/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 86.0880 - val_loss: 64.5531\n",
            "Epoch 24/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 86.0771 - val_loss: 64.5447\n",
            "Epoch 25/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 86.0661 - val_loss: 64.5363\n",
            "Epoch 26/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 86.0553 - val_loss: 64.5280\n",
            "Epoch 27/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 86.0443 - val_loss: 64.5196\n",
            "Epoch 28/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 86.0334 - val_loss: 64.5112\n",
            "Epoch 29/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 86.0225 - val_loss: 64.5028\n",
            "Epoch 30/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 86.0116 - val_loss: 64.4945\n",
            "Epoch 31/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 86.0007 - val_loss: 64.4861\n",
            "Epoch 32/500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 85.9898 - val_loss: 64.4777\n",
            "Epoch 33/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 85.9789 - val_loss: 64.4694\n",
            "Epoch 34/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 85.9680 - val_loss: 64.4610\n",
            "Epoch 35/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 85.9571 - val_loss: 64.4526\n",
            "Epoch 36/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 85.9462 - val_loss: 64.4442\n",
            "Epoch 37/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 85.9353 - val_loss: 64.4359\n",
            "Epoch 38/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 85.9244 - val_loss: 64.4275\n",
            "Epoch 39/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 85.9135 - val_loss: 64.4191\n",
            "Epoch 40/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 85.9026 - val_loss: 64.4107\n",
            "Epoch 41/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 85.8917 - val_loss: 64.4024\n",
            "Epoch 42/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 85.8808 - val_loss: 64.3940\n",
            "Epoch 43/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 85.8699 - val_loss: 64.3856\n",
            "Epoch 44/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 85.8590 - val_loss: 64.3772\n",
            "Epoch 45/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 85.8481 - val_loss: 64.3689\n",
            "Epoch 46/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 85.8372 - val_loss: 64.3605\n",
            "Epoch 47/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 85.8263 - val_loss: 64.3521\n",
            "Epoch 48/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 85.8154 - val_loss: 64.3437\n",
            "Epoch 49/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 85.8045 - val_loss: 64.3354\n",
            "Epoch 50/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 85.7935 - val_loss: 64.3270\n",
            "Epoch 51/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 85.7827 - val_loss: 64.3186\n",
            "Epoch 52/500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 85.7717 - val_loss: 64.3102\n",
            "Epoch 53/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 85.7608 - val_loss: 64.3019\n",
            "Epoch 54/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 85.7499 - val_loss: 64.2935\n",
            "Epoch 55/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 85.7390 - val_loss: 64.2851\n",
            "Epoch 56/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 85.7281 - val_loss: 64.2767\n",
            "Epoch 57/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 85.7172 - val_loss: 64.2684\n",
            "Epoch 58/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 85.7063 - val_loss: 64.2600\n",
            "Epoch 59/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 85.6954 - val_loss: 64.2516\n",
            "Epoch 60/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 85.6845 - val_loss: 64.2432\n",
            "Epoch 61/500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 85.6736 - val_loss: 64.2349\n",
            "Epoch 62/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 85.6627 - val_loss: 64.2265\n",
            "Epoch 63/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 85.6518 - val_loss: 64.2181\n",
            "Epoch 64/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 85.6409 - val_loss: 64.2097\n",
            "Epoch 65/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 85.6300 - val_loss: 64.2014\n",
            "Epoch 66/500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 85.6191 - val_loss: 64.1930\n",
            "Epoch 67/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 85.6082 - val_loss: 64.1846\n",
            "Epoch 68/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 85.5973 - val_loss: 64.1762\n",
            "Epoch 69/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 85.5864 - val_loss: 64.1679\n",
            "Epoch 70/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 85.5755 - val_loss: 64.1595\n",
            "Epoch 71/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 85.5646 - val_loss: 64.1511\n",
            "Epoch 72/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 85.5536 - val_loss: 64.1427\n",
            "Epoch 73/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 85.5428 - val_loss: 64.1344\n",
            "Epoch 74/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 85.5318 - val_loss: 64.1260\n",
            "Epoch 75/500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 85.5209 - val_loss: 64.1176\n",
            "Epoch 76/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 85.5100 - val_loss: 64.1092\n",
            "Epoch 77/500\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 85.4991 - val_loss: 64.1009\n",
            "Epoch 78/500\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 85.4882 - val_loss: 64.0925\n",
            "Epoch 79/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 85.4773 - val_loss: 64.0841\n",
            "Epoch 80/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 85.4664 - val_loss: 64.0757\n",
            "Epoch 81/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 85.4555 - val_loss: 64.0674\n",
            "Epoch 82/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 85.4446 - val_loss: 64.0590\n",
            "Epoch 83/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 85.4337 - val_loss: 64.0506\n",
            "Epoch 84/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 85.4228 - val_loss: 64.0422\n",
            "Epoch 85/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 85.4119 - val_loss: 64.0339\n",
            "Epoch 86/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 85.4010 - val_loss: 64.0255\n",
            "Epoch 87/500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 85.3901 - val_loss: 64.0171\n",
            "Epoch 88/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 85.3792 - val_loss: 64.0087\n",
            "Epoch 89/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 85.3683 - val_loss: 64.0004\n",
            "Epoch 90/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 85.3574 - val_loss: 63.9920\n",
            "Epoch 91/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 85.3465 - val_loss: 63.9836\n",
            "Epoch 92/500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 85.3356 - val_loss: 63.9752\n",
            "Epoch 93/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 85.3247 - val_loss: 63.9669\n",
            "Epoch 94/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 85.3138 - val_loss: 63.9585\n",
            "Epoch 95/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 85.3028 - val_loss: 63.9501\n",
            "Epoch 96/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 85.2919 - val_loss: 63.9417\n",
            "Epoch 97/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 85.2810 - val_loss: 63.9334\n",
            "Epoch 98/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 85.2701 - val_loss: 63.9250\n",
            "Epoch 99/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 85.2592 - val_loss: 63.9166\n",
            "Epoch 100/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 85.2483 - val_loss: 63.9082\n",
            "Epoch 101/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 85.2374 - val_loss: 63.8999\n",
            "Epoch 102/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 85.2265 - val_loss: 63.8915\n",
            "Epoch 103/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 85.2156 - val_loss: 63.8831\n",
            "Epoch 104/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 85.2047 - val_loss: 63.8747\n",
            "Epoch 105/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 85.1938 - val_loss: 63.8664\n",
            "Epoch 106/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 85.1829 - val_loss: 63.8580\n",
            "Epoch 107/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 85.1720 - val_loss: 63.8496\n",
            "Epoch 108/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 85.1611 - val_loss: 63.8412\n",
            "Epoch 109/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 85.1502 - val_loss: 63.8329\n",
            "Epoch 110/500\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 85.1393 - val_loss: 63.8245\n",
            "Epoch 111/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 85.1284 - val_loss: 63.8161\n",
            "Epoch 112/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 85.1175 - val_loss: 63.8078\n",
            "Epoch 113/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 85.1066 - val_loss: 63.7994\n",
            "Epoch 114/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 85.0957 - val_loss: 63.7910\n",
            "Epoch 115/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 85.0848 - val_loss: 63.7826\n",
            "Epoch 116/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 85.0739 - val_loss: 63.7743\n",
            "Epoch 117/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 85.0629 - val_loss: 63.7659\n",
            "Epoch 118/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 85.0520 - val_loss: 63.7575\n",
            "Epoch 119/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 85.0411 - val_loss: 63.7491\n",
            "Epoch 120/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 85.0302 - val_loss: 63.7407\n",
            "Epoch 121/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 85.0193 - val_loss: 63.7324\n",
            "Epoch 122/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 85.0084 - val_loss: 63.7240\n",
            "Epoch 123/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 84.9975 - val_loss: 63.7156\n",
            "Epoch 124/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 84.9866 - val_loss: 63.7073\n",
            "Epoch 125/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 84.9757 - val_loss: 63.6989\n",
            "Epoch 126/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 84.9648 - val_loss: 63.6905\n",
            "Epoch 127/500\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 84.9539 - val_loss: 63.6821\n",
            "Epoch 128/500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 84.9430 - val_loss: 63.6738\n",
            "Epoch 129/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 84.9321 - val_loss: 63.6654\n",
            "Epoch 130/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 84.9212 - val_loss: 63.6570\n",
            "Epoch 131/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 84.9103 - val_loss: 63.6486\n",
            "Epoch 132/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 84.8994 - val_loss: 63.6402\n",
            "Epoch 133/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 84.8885 - val_loss: 63.6319\n",
            "Epoch 134/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 84.8775 - val_loss: 63.6235\n",
            "Epoch 135/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 84.8667 - val_loss: 63.6151\n",
            "Epoch 136/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 84.8558 - val_loss: 63.6068\n",
            "Epoch 137/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 84.8448 - val_loss: 63.5984\n",
            "Epoch 138/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 84.8339 - val_loss: 63.5900\n",
            "Epoch 139/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 84.8230 - val_loss: 63.5816\n",
            "Epoch 140/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 84.8121 - val_loss: 63.5732\n",
            "Epoch 141/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 84.8012 - val_loss: 63.5649\n",
            "Epoch 142/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 84.7903 - val_loss: 63.5565\n",
            "Epoch 143/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 84.7794 - val_loss: 63.5481\n",
            "Epoch 144/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 84.7685 - val_loss: 63.5397\n",
            "Epoch 145/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 84.7576 - val_loss: 63.5314\n",
            "Epoch 146/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 84.7467 - val_loss: 63.5230\n",
            "Epoch 147/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 84.7358 - val_loss: 63.5146\n",
            "Epoch 148/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 84.7249 - val_loss: 63.5062\n",
            "Epoch 149/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 84.7140 - val_loss: 63.4979\n",
            "Epoch 150/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 84.7031 - val_loss: 63.4895\n",
            "Epoch 151/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 84.6922 - val_loss: 63.4811\n",
            "Epoch 152/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 84.6813 - val_loss: 63.4728\n",
            "Epoch 153/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 84.6704 - val_loss: 63.4644\n",
            "Epoch 154/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 84.6595 - val_loss: 63.4560\n",
            "Epoch 155/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 84.6486 - val_loss: 63.4476\n",
            "Epoch 156/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 84.6377 - val_loss: 63.4392\n",
            "Epoch 157/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 84.6267 - val_loss: 63.4309\n",
            "Epoch 158/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 84.6158 - val_loss: 63.4225\n",
            "Epoch 159/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 84.6049 - val_loss: 63.4141\n",
            "Epoch 160/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 84.5940 - val_loss: 63.4057\n",
            "Epoch 161/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 84.5831 - val_loss: 63.3974\n",
            "Epoch 162/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 84.5722 - val_loss: 63.3890\n",
            "Epoch 163/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 84.5613 - val_loss: 63.3806\n",
            "Epoch 164/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 84.5504 - val_loss: 63.3722\n",
            "Epoch 165/500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 84.5395 - val_loss: 63.3639\n",
            "Epoch 166/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 84.5286 - val_loss: 63.3555\n",
            "Epoch 167/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 84.5177 - val_loss: 63.3471\n",
            "Epoch 168/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 84.5068 - val_loss: 63.3387\n",
            "Epoch 169/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 84.4959 - val_loss: 63.3304\n",
            "Epoch 170/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 84.4850 - val_loss: 63.3220\n",
            "Epoch 171/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 84.4741 - val_loss: 63.3136\n",
            "Epoch 172/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 84.4632 - val_loss: 63.3052\n",
            "Epoch 173/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 84.4523 - val_loss: 63.2969\n",
            "Epoch 174/500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 84.4414 - val_loss: 63.2885\n",
            "Epoch 175/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 84.4305 - val_loss: 63.2801\n",
            "Epoch 176/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 84.4195 - val_loss: 63.2717\n",
            "Epoch 177/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 84.4086 - val_loss: 63.2634\n",
            "Epoch 178/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 84.3977 - val_loss: 63.2550\n",
            "Epoch 179/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 84.3868 - val_loss: 63.2466\n",
            "Epoch 180/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 84.3759 - val_loss: 63.2382\n",
            "Epoch 181/500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 84.3650 - val_loss: 63.2299\n",
            "Epoch 182/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 84.3541 - val_loss: 63.2215\n",
            "Epoch 183/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 84.3432 - val_loss: 63.2131\n",
            "Epoch 184/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 84.3323 - val_loss: 63.2047\n",
            "Epoch 185/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 84.3214 - val_loss: 63.1964\n",
            "Epoch 186/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 84.3105 - val_loss: 63.1880\n",
            "Epoch 187/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 84.2996 - val_loss: 63.1796\n",
            "Epoch 188/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 84.2887 - val_loss: 63.1712\n",
            "Epoch 189/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 84.2778 - val_loss: 63.1629\n",
            "Epoch 190/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 84.2669 - val_loss: 63.1545\n",
            "Epoch 191/500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 84.2560 - val_loss: 63.1461\n",
            "Epoch 192/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 84.2451 - val_loss: 63.1377\n",
            "Epoch 193/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 84.2342 - val_loss: 63.1294\n",
            "Epoch 194/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 84.2233 - val_loss: 63.1210\n",
            "Epoch 195/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 84.2124 - val_loss: 63.1126\n",
            "Epoch 196/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 84.2015 - val_loss: 63.1042\n",
            "Epoch 197/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 84.1905 - val_loss: 63.0959\n",
            "Epoch 198/500\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 84.1796 - val_loss: 63.0875\n",
            "Epoch 199/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 84.1687 - val_loss: 63.0791\n",
            "Epoch 200/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 84.1578 - val_loss: 63.0707\n",
            "Epoch 201/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 84.1469 - val_loss: 63.0624\n",
            "Epoch 202/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 84.1360 - val_loss: 63.0540\n",
            "Epoch 203/500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 84.1251 - val_loss: 63.0456\n",
            "Epoch 204/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 84.1142 - val_loss: 63.0372\n",
            "Epoch 205/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 84.1033 - val_loss: 63.0289\n",
            "Epoch 206/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 84.0924 - val_loss: 63.0205\n",
            "Epoch 207/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 84.0815 - val_loss: 63.0121\n",
            "Epoch 208/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 84.0706 - val_loss: 63.0037\n",
            "Epoch 209/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 84.0597 - val_loss: 62.9954\n",
            "Epoch 210/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 84.0488 - val_loss: 62.9870\n",
            "Epoch 211/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 84.0379 - val_loss: 62.9786\n",
            "Epoch 212/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 84.0270 - val_loss: 62.9702\n",
            "Epoch 213/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 84.0161 - val_loss: 62.9619\n",
            "Epoch 214/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 84.0052 - val_loss: 62.9535\n",
            "Epoch 215/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 83.9943 - val_loss: 62.9451\n",
            "Epoch 216/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 83.9834 - val_loss: 62.9367\n",
            "Epoch 217/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 83.9725 - val_loss: 62.9284\n",
            "Epoch 218/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 83.9615 - val_loss: 62.9200\n",
            "Epoch 219/500\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 83.9506 - val_loss: 62.9116\n",
            "Epoch 220/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 83.9397 - val_loss: 62.9032\n",
            "Epoch 221/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 83.9288 - val_loss: 62.8949\n",
            "Epoch 222/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 83.9179 - val_loss: 62.8865\n",
            "Epoch 223/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 83.9070 - val_loss: 62.8781\n",
            "Epoch 224/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 83.8961 - val_loss: 62.8697\n",
            "Epoch 225/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 83.8852 - val_loss: 62.8614\n",
            "Epoch 226/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 83.8743 - val_loss: 62.8530\n",
            "Epoch 227/500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 83.8634 - val_loss: 62.8446\n",
            "Epoch 228/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 83.8525 - val_loss: 62.8362\n",
            "Epoch 229/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 83.8416 - val_loss: 62.8279\n",
            "Epoch 230/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 83.8307 - val_loss: 62.8195\n",
            "Epoch 231/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 83.8198 - val_loss: 62.8111\n",
            "Epoch 232/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 83.8089 - val_loss: 62.8027\n",
            "Epoch 233/500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 83.7980 - val_loss: 62.7944\n",
            "Epoch 234/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 83.7871 - val_loss: 62.7860\n",
            "Epoch 235/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 83.7762 - val_loss: 62.7776\n",
            "Epoch 236/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 83.7653 - val_loss: 62.7692\n",
            "Epoch 237/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 83.7543 - val_loss: 62.7609\n",
            "Epoch 238/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 83.7434 - val_loss: 62.7525\n",
            "Epoch 239/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 83.7325 - val_loss: 62.7441\n",
            "Epoch 240/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 83.7216 - val_loss: 62.7357\n",
            "Epoch 241/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 83.7107 - val_loss: 62.7274\n",
            "Epoch 242/500\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 83.6998 - val_loss: 62.7190\n",
            "Epoch 243/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 83.6889 - val_loss: 62.7106\n",
            "Epoch 244/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 83.6780 - val_loss: 62.7022\n",
            "Epoch 245/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 83.6671 - val_loss: 62.6939\n",
            "Epoch 246/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 83.6562 - val_loss: 62.6855\n",
            "Epoch 247/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 83.6453 - val_loss: 62.6771\n",
            "Epoch 248/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 83.6344 - val_loss: 62.6687\n",
            "Epoch 249/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 83.6235 - val_loss: 62.6604\n",
            "Epoch 250/500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 83.6126 - val_loss: 62.6520\n",
            "Epoch 251/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 83.6017 - val_loss: 62.6436\n",
            "Epoch 252/500\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 83.5908 - val_loss: 62.6352\n",
            "Epoch 253/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 83.5799 - val_loss: 62.6269\n",
            "Epoch 254/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 83.5690 - val_loss: 62.6185\n",
            "Epoch 255/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 83.5581 - val_loss: 62.6101\n",
            "Epoch 256/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 83.5472 - val_loss: 62.6017\n",
            "Epoch 257/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 83.5362 - val_loss: 62.5934\n",
            "Epoch 258/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 83.5253 - val_loss: 62.5850\n",
            "Epoch 259/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 83.5144 - val_loss: 62.5766\n",
            "Epoch 260/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 83.5035 - val_loss: 62.5682\n",
            "Epoch 261/500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 83.4926 - val_loss: 62.5599\n",
            "Epoch 262/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 83.4817 - val_loss: 62.5515\n",
            "Epoch 263/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 83.4708 - val_loss: 62.5431\n",
            "Epoch 264/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 83.4599 - val_loss: 62.5347\n",
            "Epoch 265/500\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 83.4490 - val_loss: 62.5264\n",
            "Epoch 266/500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 83.4381 - val_loss: 62.5180\n",
            "Epoch 267/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 83.4272 - val_loss: 62.5096\n",
            "Epoch 268/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 83.4163 - val_loss: 62.5012\n",
            "Epoch 269/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 83.4054 - val_loss: 62.4929\n",
            "Epoch 270/500\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 83.3945 - val_loss: 62.4845\n",
            "Epoch 271/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 83.3836 - val_loss: 62.4761\n",
            "Epoch 272/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 83.3727 - val_loss: 62.4677\n",
            "Epoch 273/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 83.3618 - val_loss: 62.4594\n",
            "Epoch 274/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 83.3509 - val_loss: 62.4510\n",
            "Epoch 275/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 83.3400 - val_loss: 62.4426\n",
            "Epoch 276/500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 83.3290 - val_loss: 62.4342\n",
            "Epoch 277/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 83.3181 - val_loss: 62.4258\n",
            "Epoch 278/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 83.3072 - val_loss: 62.4175\n",
            "Epoch 279/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 83.2963 - val_loss: 62.4091\n",
            "Epoch 280/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 83.2854 - val_loss: 62.4007\n",
            "Epoch 281/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 83.2745 - val_loss: 62.3924\n",
            "Epoch 282/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 83.2636 - val_loss: 62.3840\n",
            "Epoch 283/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 83.2527 - val_loss: 62.3756\n",
            "Epoch 284/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 83.2418 - val_loss: 62.3672\n",
            "Epoch 285/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 83.2309 - val_loss: 62.3589\n",
            "Epoch 286/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 83.2200 - val_loss: 62.3505\n",
            "Epoch 287/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 83.2091 - val_loss: 62.3421\n",
            "Epoch 288/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 83.1982 - val_loss: 62.3337\n",
            "Epoch 289/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 83.1873 - val_loss: 62.3254\n",
            "Epoch 290/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 83.1764 - val_loss: 62.3170\n",
            "Epoch 291/500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 83.1655 - val_loss: 62.3086\n",
            "Epoch 292/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 83.1546 - val_loss: 62.3002\n",
            "Epoch 293/500\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 83.1437 - val_loss: 62.2918\n",
            "Epoch 294/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 83.1328 - val_loss: 62.2835\n",
            "Epoch 295/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 83.1219 - val_loss: 62.2751\n",
            "Epoch 296/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 83.1109 - val_loss: 62.2667\n",
            "Epoch 297/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 83.1000 - val_loss: 62.2584\n",
            "Epoch 298/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 83.0891 - val_loss: 62.2500\n",
            "Epoch 299/500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 83.0782 - val_loss: 62.2416\n",
            "Epoch 300/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 83.0673 - val_loss: 62.2332\n",
            "Epoch 301/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 83.0564 - val_loss: 62.2248\n",
            "Epoch 302/500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 83.0455 - val_loss: 62.2165\n",
            "Epoch 303/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 83.0346 - val_loss: 62.2081\n",
            "Epoch 304/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 83.0237 - val_loss: 62.1997\n",
            "Epoch 305/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 83.0128 - val_loss: 62.1913\n",
            "Epoch 306/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 83.0019 - val_loss: 62.1830\n",
            "Epoch 307/500\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 82.9910 - val_loss: 62.1746\n",
            "Epoch 308/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 82.9801 - val_loss: 62.1662\n",
            "Epoch 309/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 82.9692 - val_loss: 62.1578\n",
            "Epoch 310/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 82.9583 - val_loss: 62.1495\n",
            "Epoch 311/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 82.9474 - val_loss: 62.1411\n",
            "Epoch 312/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 82.9365 - val_loss: 62.1327\n",
            "Epoch 313/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 82.9256 - val_loss: 62.1243\n",
            "Epoch 314/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 82.9147 - val_loss: 62.1160\n",
            "Epoch 315/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 82.9037 - val_loss: 62.1076\n",
            "Epoch 316/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 82.8928 - val_loss: 62.0992\n",
            "Epoch 317/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 82.8819 - val_loss: 62.0908\n",
            "Epoch 318/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 82.8710 - val_loss: 62.0825\n",
            "Epoch 319/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 82.8601 - val_loss: 62.0741\n",
            "Epoch 320/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 82.8492 - val_loss: 62.0657\n",
            "Epoch 321/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 82.8383 - val_loss: 62.0573\n",
            "Epoch 322/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 82.8274 - val_loss: 62.0490\n",
            "Epoch 323/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 82.8165 - val_loss: 62.0406\n",
            "Epoch 324/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 82.8056 - val_loss: 62.0322\n",
            "Epoch 325/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 82.7947 - val_loss: 62.0238\n",
            "Epoch 326/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 82.7838 - val_loss: 62.0155\n",
            "Epoch 327/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 82.7729 - val_loss: 62.0071\n",
            "Epoch 328/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 82.7620 - val_loss: 61.9987\n",
            "Epoch 329/500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 82.7511 - val_loss: 61.9903\n",
            "Epoch 330/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 82.7402 - val_loss: 61.9820\n",
            "Epoch 331/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 82.7293 - val_loss: 61.9736\n",
            "Epoch 332/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 82.7184 - val_loss: 61.9652\n",
            "Epoch 333/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 82.7075 - val_loss: 61.9568\n",
            "Epoch 334/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 82.6965 - val_loss: 61.9485\n",
            "Epoch 335/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 82.6857 - val_loss: 61.9401\n",
            "Epoch 336/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 82.6747 - val_loss: 61.9317\n",
            "Epoch 337/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 82.6638 - val_loss: 61.9233\n",
            "Epoch 338/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 82.6529 - val_loss: 61.9150\n",
            "Epoch 339/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 82.6420 - val_loss: 61.9066\n",
            "Epoch 340/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 82.6311 - val_loss: 61.8982\n",
            "Epoch 341/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 82.6202 - val_loss: 61.8898\n",
            "Epoch 342/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 82.6093 - val_loss: 61.8815\n",
            "Epoch 343/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 82.5984 - val_loss: 61.8731\n",
            "Epoch 344/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 82.5875 - val_loss: 61.8647\n",
            "Epoch 345/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 82.5766 - val_loss: 61.8563\n",
            "Epoch 346/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 82.5657 - val_loss: 61.8480\n",
            "Epoch 347/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 82.5548 - val_loss: 61.8396\n",
            "Epoch 348/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 82.5439 - val_loss: 61.8312\n",
            "Epoch 349/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 82.5330 - val_loss: 61.8228\n",
            "Epoch 350/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 82.5221 - val_loss: 61.8145\n",
            "Epoch 351/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 82.5112 - val_loss: 61.8061\n",
            "Epoch 352/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 82.5003 - val_loss: 61.7977\n",
            "Epoch 353/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 82.4893 - val_loss: 61.7893\n",
            "Epoch 354/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 82.4785 - val_loss: 61.7810\n",
            "Epoch 355/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 82.4675 - val_loss: 61.7726\n",
            "Epoch 356/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 82.4566 - val_loss: 61.7642\n",
            "Epoch 357/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 82.4457 - val_loss: 61.7558\n",
            "Epoch 358/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 82.4348 - val_loss: 61.7475\n",
            "Epoch 359/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 82.4239 - val_loss: 61.7391\n",
            "Epoch 360/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 82.4130 - val_loss: 61.7307\n",
            "Epoch 361/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 82.4021 - val_loss: 61.7223\n",
            "Epoch 362/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 82.3912 - val_loss: 61.7140\n",
            "Epoch 363/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 82.3803 - val_loss: 61.7056\n",
            "Epoch 364/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 82.3694 - val_loss: 61.6972\n",
            "Epoch 365/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 82.3585 - val_loss: 61.6888\n",
            "Epoch 366/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 82.3476 - val_loss: 61.6805\n",
            "Epoch 367/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 82.3367 - val_loss: 61.6721\n",
            "Epoch 368/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 82.3258 - val_loss: 61.6637\n",
            "Epoch 369/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 82.3149 - val_loss: 61.6553\n",
            "Epoch 370/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 82.3040 - val_loss: 61.6470\n",
            "Epoch 371/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 82.2931 - val_loss: 61.6386\n",
            "Epoch 372/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 82.2822 - val_loss: 61.6302\n",
            "Epoch 373/500\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 82.2713 - val_loss: 61.6218\n",
            "Epoch 374/500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 82.2603 - val_loss: 61.6135\n",
            "Epoch 375/500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 82.2494 - val_loss: 61.6051\n",
            "Epoch 376/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 82.2385 - val_loss: 61.5967\n",
            "Epoch 377/500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 82.2276 - val_loss: 61.5883\n",
            "Epoch 378/500\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 82.2167 - val_loss: 61.5800\n",
            "Epoch 379/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 82.2058 - val_loss: 61.5716\n",
            "Epoch 380/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 82.1949 - val_loss: 61.5632\n",
            "Epoch 381/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 82.1840 - val_loss: 61.5548\n",
            "Epoch 382/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 82.1731 - val_loss: 61.5465\n",
            "Epoch 383/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 82.1622 - val_loss: 61.5381\n",
            "Epoch 384/500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 82.1513 - val_loss: 61.5297\n",
            "Epoch 385/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 82.1404 - val_loss: 61.5213\n",
            "Epoch 386/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 82.1295 - val_loss: 61.5130\n",
            "Epoch 387/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 82.1186 - val_loss: 61.5046\n",
            "Epoch 388/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 82.1077 - val_loss: 61.4962\n",
            "Epoch 389/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 82.0968 - val_loss: 61.4878\n",
            "Epoch 390/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 82.0859 - val_loss: 61.4795\n",
            "Epoch 391/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 82.0750 - val_loss: 61.4711\n",
            "Epoch 392/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 82.0641 - val_loss: 61.4627\n",
            "Epoch 393/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 82.0532 - val_loss: 61.4543\n",
            "Epoch 394/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 82.0423 - val_loss: 61.4460\n",
            "Epoch 395/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 82.0313 - val_loss: 61.4376\n",
            "Epoch 396/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 82.0204 - val_loss: 61.4292\n",
            "Epoch 397/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 82.0095 - val_loss: 61.4208\n",
            "Epoch 398/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 81.9986 - val_loss: 61.4125\n",
            "Epoch 399/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 81.9877 - val_loss: 61.4041\n",
            "Epoch 400/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 81.9768 - val_loss: 61.3957\n",
            "Epoch 401/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 81.9659 - val_loss: 61.3873\n",
            "Epoch 402/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 81.9550 - val_loss: 61.3790\n",
            "Epoch 403/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 81.9441 - val_loss: 61.3706\n",
            "Epoch 404/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 81.9332 - val_loss: 61.3622\n",
            "Epoch 405/500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 81.9223 - val_loss: 61.3538\n",
            "Epoch 406/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 81.9114 - val_loss: 61.3454\n",
            "Epoch 407/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 81.9005 - val_loss: 61.3371\n",
            "Epoch 408/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 81.8896 - val_loss: 61.3287\n",
            "Epoch 409/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 81.8787 - val_loss: 61.3203\n",
            "Epoch 410/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 81.8678 - val_loss: 61.3120\n",
            "Epoch 411/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 81.8569 - val_loss: 61.3036\n",
            "Epoch 412/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 81.8459 - val_loss: 61.2952\n",
            "Epoch 413/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 81.8350 - val_loss: 61.2868\n",
            "Epoch 414/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 81.8242 - val_loss: 61.2785\n",
            "Epoch 415/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 81.8132 - val_loss: 61.2701\n",
            "Epoch 416/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 81.8023 - val_loss: 61.2617\n",
            "Epoch 417/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 81.7914 - val_loss: 61.2533\n",
            "Epoch 418/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 81.7805 - val_loss: 61.2449\n",
            "Epoch 419/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 81.7696 - val_loss: 61.2366\n",
            "Epoch 420/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 81.7587 - val_loss: 61.2282\n",
            "Epoch 421/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 81.7478 - val_loss: 61.2198\n",
            "Epoch 422/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 81.7369 - val_loss: 61.2114\n",
            "Epoch 423/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 81.7260 - val_loss: 61.2031\n",
            "Epoch 424/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 81.7151 - val_loss: 61.1947\n",
            "Epoch 425/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 81.7042 - val_loss: 61.1863\n",
            "Epoch 426/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 81.6933 - val_loss: 61.1779\n",
            "Epoch 427/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 81.6824 - val_loss: 61.1696\n",
            "Epoch 428/500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 81.6715 - val_loss: 61.1612\n",
            "Epoch 429/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 81.6606 - val_loss: 61.1528\n",
            "Epoch 430/500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 81.6497 - val_loss: 61.1444\n",
            "Epoch 431/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 81.6388 - val_loss: 61.1361\n",
            "Epoch 432/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 81.6279 - val_loss: 61.1277\n",
            "Epoch 433/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 81.6169 - val_loss: 61.1193\n",
            "Epoch 434/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 81.6060 - val_loss: 61.1109\n",
            "Epoch 435/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 81.5951 - val_loss: 61.1026\n",
            "Epoch 436/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 81.5842 - val_loss: 61.0942\n",
            "Epoch 437/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 81.5733 - val_loss: 61.0858\n",
            "Epoch 438/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 81.5624 - val_loss: 61.0774\n",
            "Epoch 439/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 81.5515 - val_loss: 61.0691\n",
            "Epoch 440/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 81.5406 - val_loss: 61.0607\n",
            "Epoch 441/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 81.5297 - val_loss: 61.0523\n",
            "Epoch 442/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 81.5188 - val_loss: 61.0439\n",
            "Epoch 443/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 81.5079 - val_loss: 61.0356\n",
            "Epoch 444/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 81.4970 - val_loss: 61.0272\n",
            "Epoch 445/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 81.4861 - val_loss: 61.0188\n",
            "Epoch 446/500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 81.4752 - val_loss: 61.0104\n",
            "Epoch 447/500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 81.4643 - val_loss: 61.0021\n",
            "Epoch 448/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 81.4534 - val_loss: 60.9937\n",
            "Epoch 449/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 81.4425 - val_loss: 60.9853\n",
            "Epoch 450/500\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 81.4316 - val_loss: 60.9769\n",
            "Epoch 451/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 81.4206 - val_loss: 60.9686\n",
            "Epoch 452/500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 81.4098 - val_loss: 60.9602\n",
            "Epoch 453/500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 81.3988 - val_loss: 60.9518\n",
            "Epoch 454/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 81.3879 - val_loss: 60.9434\n",
            "Epoch 455/500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 81.3770 - val_loss: 60.9351\n",
            "Epoch 456/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 81.3661 - val_loss: 60.9267\n",
            "Epoch 457/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 81.3552 - val_loss: 60.9183\n",
            "Epoch 458/500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 81.3443 - val_loss: 60.9099\n",
            "Epoch 459/500\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 81.3334 - val_loss: 60.9016\n",
            "Epoch 460/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 81.3225 - val_loss: 60.8932\n",
            "Epoch 461/500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 81.3116 - val_loss: 60.8848\n",
            "Epoch 462/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 81.3007 - val_loss: 60.8764\n",
            "Epoch 463/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 81.2898 - val_loss: 60.8681\n",
            "Epoch 464/500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 81.2789 - val_loss: 60.8597\n",
            "Epoch 465/500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 81.2680 - val_loss: 60.8513\n",
            "Epoch 466/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 81.2571 - val_loss: 60.8429\n",
            "Epoch 467/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 81.2462 - val_loss: 60.8346\n",
            "Epoch 468/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 81.2353 - val_loss: 60.8262\n",
            "Epoch 469/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 81.2243 - val_loss: 60.8178\n",
            "Epoch 470/500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 81.2135 - val_loss: 60.8094\n",
            "Epoch 471/500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 81.2026 - val_loss: 60.8011\n",
            "Epoch 472/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 81.1917 - val_loss: 60.7927\n",
            "Epoch 473/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 81.1807 - val_loss: 60.7843\n",
            "Epoch 474/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 81.1698 - val_loss: 60.7759\n",
            "Epoch 475/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 81.1589 - val_loss: 60.7676\n",
            "Epoch 476/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 81.1480 - val_loss: 60.7592\n",
            "Epoch 477/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 81.1371 - val_loss: 60.7508\n",
            "Epoch 478/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 81.1262 - val_loss: 60.7424\n",
            "Epoch 479/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 81.1153 - val_loss: 60.7341\n",
            "Epoch 480/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 81.1044 - val_loss: 60.7257\n",
            "Epoch 481/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 81.0935 - val_loss: 60.7173\n",
            "Epoch 482/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 81.0826 - val_loss: 60.7089\n",
            "Epoch 483/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 81.0717 - val_loss: 60.7006\n",
            "Epoch 484/500\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 81.0608 - val_loss: 60.6922\n",
            "Epoch 485/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 81.0499 - val_loss: 60.6838\n",
            "Epoch 486/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 81.0390 - val_loss: 60.6754\n",
            "Epoch 487/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 81.0281 - val_loss: 60.6671\n",
            "Epoch 488/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 81.0172 - val_loss: 60.6587\n",
            "Epoch 489/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 81.0063 - val_loss: 60.6503\n",
            "Epoch 490/500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 80.9953 - val_loss: 60.6419\n",
            "Epoch 491/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 80.9845 - val_loss: 60.6336\n",
            "Epoch 492/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 80.9735 - val_loss: 60.6252\n",
            "Epoch 493/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 80.9626 - val_loss: 60.6168\n",
            "Epoch 494/500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 80.9517 - val_loss: 60.6084\n",
            "Epoch 495/500\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 80.9408 - val_loss: 60.6000\n",
            "Epoch 496/500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 80.9299 - val_loss: 60.5917\n",
            "Epoch 497/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 80.9190 - val_loss: 60.5833\n",
            "Epoch 498/500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 80.9081 - val_loss: 60.5749\n",
            "Epoch 499/500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 80.8972 - val_loss: 60.5666\n",
            "Epoch 500/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 80.8863 - val_loss: 60.5582\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f2964c37710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hn_RIF2vlD84"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}